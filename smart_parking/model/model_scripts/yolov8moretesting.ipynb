{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import PIL\n",
    "import pathlib\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\model_images\")\n",
    "valid_dir = pathlib.Path(r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\valid_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=data_dir,\n",
    "    target_size=(180, 180),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_dir,\n",
    "    target_size=(180, 180),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Augmentation\n",
    "model.add(keras.layers.RandomRotation(0.2))\n",
    "model.add(keras.layers.RandomContrast(0.5))\n",
    "model.add(keras.layers.RandomZoom(0.3))\n",
    "# CNN 1\n",
    "model.add(keras.layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu',padding='same',input_shape=(180,180,3)))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# CNN 2\n",
    "model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# CNN 3\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# Dense\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(256,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator,validation_data=valid_datagen ,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmentation block (cleanly separated)\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomContrast(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "# ], name=\"augmentation\")\n",
    "\n",
    "# # Build the model\n",
    "# model = models.Sequential([\n",
    "#     data_augmentation,\n",
    "    \n",
    "#     # Conv Block 1\n",
    "#     layers.Conv2D(16, (3, 3), padding='same', input_shape=(img_height, img_width, 3)),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Conv Block 2\n",
    "#     layers.Conv2D(32, (3, 3), padding='same'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Conv Block 3\n",
    "#     layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Dense Layers\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Callbacks\n",
    "# early_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(\n",
    "#     train_generator,\n",
    "#     validation_data=valid_generator,\n",
    "#     epochs=20,\n",
    "#     callbacks=[early_stop, checkpoint]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the model\n",
    "class ParkingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParkingModel, self).__init__()\n",
    "\n",
    "        # Augmentation is done via transforms in PyTorch, not in the model\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 22 * 22, 256)  # 180x180 image downsampled by maxpool\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # For Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "\n",
    "        x = x.view(-1, 64 * 22 * 22)  # Flatten the tensor\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Output layer\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ParkingModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define data augmentation and preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(180, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((180, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = datasets.ImageFolder(root=r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\model_images\", transform=train_transform)\n",
    "valid_dataset = datasets.ImageFolder(root=r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\valid_images\", transform=valid_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "#     # Save best model\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), \"best_model.pth\")\n",
    "#         print(\"Saved best model\")\n",
    "\n",
    "# # Final save\n",
    "# torch.save(model.state_dict(), \"final_parking_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        predicted = outputs.round()\n",
    "        correct_predictions += (predicted.squeeze() == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = 100 * correct_predictions / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            predicted = outputs.round()\n",
    "            correct_predictions += (predicted.squeeze() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    valid_accuracy = 100 * correct_predictions / total\n",
    "    print(f\"Validation Loss: {valid_loss/len(valid_loader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
