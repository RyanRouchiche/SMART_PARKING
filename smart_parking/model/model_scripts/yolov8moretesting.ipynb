<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():   \n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"weights\\yolov8l.pt\")\n",
    "# model_v12 = YOLO(\"weights/yolov8l.pt\")\n",
    "model_v11 = YOLO(\"yolov8n_fold_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 f:\\Python\\yolov8\\birdviewimg\\imageparking2.jpg: 1920x1920 34 cars, 228.4ms\n",
      "Speed: 46.0ms preprocess, 228.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1920, 1920)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, label_text, (x1, y1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Display the frame with bounding boxes\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYOLOv8 Detection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Release the video capture object and close all OpenCV windows\u001b[39;00m\n\u001b[0;32m     33\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mf:\\Python\\yolov8\\.venv\\lib\\site-packages\\ultralytics\\utils\\patches.py:56\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(winname, mat)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimshow\u001b[39m(winname: \u001b[38;5;28mstr\u001b[39m, mat: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Displays an image in the specified window.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m        mat (np.ndarray): Image to be shown.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[43m_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43municode_escape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# image = cv2.imread(\"birdviewimg\\\\1_Spot_3.jpg\")\n",
    "source = 'birdviewimg\\parking_1920_1080.mp4'\n",
    "cap = cv2.VideoCapture(source)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "success, frame = cap.read()\n",
    "\n",
    "results_v11 = model_v11(\"birdviewimg\\\\imageparking2.jpg\")\n",
    "# for result in results_v11:\n",
    "#     result.show()\n",
    "\n",
    "for result in results_v11:\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()  # Bounding boxes in xyxy format\n",
    "    scores = result.boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    labels = result.boxes.cls.cpu().numpy()  # Class labels\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Extract box coordinates\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Put the label and confidence score on the bounding box\n",
    "        label_text = f\"{model.names[int(label)]}: {score:.2f}\"\n",
    "        cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the frame with bounding boxes\n",
    "cv2.imshow(\"YOLOv8 Detection\", frame)\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# results = model(image)\n",
    "\n",
    "# results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parking_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import PIL\n",
    "import pathlib\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\model_images\\train_images\")\n",
    "valid_dir = pathlib.Path(r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\model_images\\valid_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=data_dir,\n",
    "    target_size=(180, 180),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_dir,\n",
    "    target_size=(180, 180),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Augmentation\n",
    "model.add(keras.layers.RandomRotation(0.2))\n",
    "model.add(keras.layers.RandomContrast(0.5))\n",
    "model.add(keras.layers.RandomZoom(0.3))\n",
    "# CNN 1\n",
    "model.add(keras.layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu',padding='same',input_shape=(180,180,3)))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# CNN 2\n",
    "model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# CNN 3\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPool2D())\n",
    "# Dense\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(256,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator,validation_data=valid_datagen ,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmentation block (cleanly separated)\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomContrast(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "# ], name=\"augmentation\")\n",
    "\n",
    "# # Build the model\n",
    "# model = models.Sequential([\n",
    "#     data_augmentation,\n",
    "    \n",
    "#     # Conv Block 1\n",
    "#     layers.Conv2D(16, (3, 3), padding='same', input_shape=(img_height, img_width, 3)),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Conv Block 2\n",
    "#     layers.Conv2D(32, (3, 3), padding='same'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Conv Block 3\n",
    "#     layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "\n",
    "#     # Dense Layers\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Callbacks\n",
    "# early_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(\n",
    "#     train_generator,\n",
    "#     validation_data=valid_generator,\n",
    "#     epochs=20,\n",
    "#     callbacks=[early_stop, checkpoint]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Define the model\n",
    "# class ParkingModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ParkingModel, self).__init__()\n",
    "\n",
    "#         # Augmentation is done via transforms in PyTorch, not in the model\n",
    "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.fc1 = nn.Linear(64 * 22 * 22, 256)  # 180x180 image downsampled by maxpool\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#         # For Batch Normalization\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.bn1(self.conv1(x)))\n",
    "#         x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "#         x = torch.relu(self.bn2(self.conv2(x)))\n",
    "#         x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "#         x = torch.relu(self.bn3(self.conv3(x)))\n",
    "#         x = torch.max_pool2d(x, 2)  # MaxPool layer\n",
    "\n",
    "#         x = x.view(-1, 64 * 22 * 22)  # Flatten the tensor\n",
    "\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))  # Output layer\n",
    "\n",
    "#         return x\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = ParkingModel()\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# # Define data augmentation and preprocessing\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomResizedCrop(180, scale=(0.8, 1.0)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "\n",
    "# valid_transform = transforms.Compose([\n",
    "#     transforms.Resize((180, 180)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "\n",
    "# # Load the datasets\n",
    "# train_dataset = datasets.ImageFolder(root=r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\model_images\", transform=train_transform)\n",
    "# valid_dataset = datasets.ImageFolder(root=r\"F:\\Github\\SMART_PARKING\\smart_parking\\model\\valid_images\", transform=valid_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# #     # Save best model\n",
    "# #     if valid_loss < best_valid_loss:\n",
    "# #         best_valid_loss = valid_loss\n",
    "# #         torch.save(model.state_dict(), \"best_model.pth\")\n",
    "# #         print(\"Saved best model\")\n",
    "\n",
    "# # # Final save\n",
    "# # torch.save(model.state_dict(), \"final_parking_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# num_epochs = 20\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs.squeeze(), labels.float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Accuracy\n",
    "#         predicted = outputs.round()\n",
    "#         correct_predictions += (predicted.squeeze() == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#     # Calculate training accuracy\n",
    "#     train_accuracy = 100 * correct_predictions / total\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     valid_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in valid_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "#             valid_loss += loss.item()\n",
    "\n",
    "#             # Accuracy\n",
    "#             predicted = outputs.round()\n",
    "#             correct_predictions += (predicted.squeeze() == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     valid_accuracy = 100 * correct_predictions / total\n",
    "#     print(f\"Validation Loss: {valid_loss/len(valid_loader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> b74f8c28c0ebb581000a63e57cc841f6eea8d159
